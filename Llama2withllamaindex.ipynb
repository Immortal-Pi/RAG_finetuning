{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG system using Llama2 with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pypdf \n",
    "import transformers\n",
    "import accelerate\n",
    "import langchain\n",
    "import torch\n",
    "import bitsandbytes\n",
    "import einops\n",
    "import llama_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Embedding \n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all the pdf's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "# SErvice context combines llama2 model with the prompt \n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='b82f99c4-44cb-4c4f-b5d3-888a97dd3599', embedding=None, metadata={'page_label': '1', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convolutional Neural Networks (CNN)  \\nA Convolutional Neural Network (CNN)  is a type of neural network that is specifically designed  to work \\nwell with images and spatial data . It makes certain assumptions about the structure of the input (like an \\nimage being made up of pixels in a grid) and uses specialized layers to process this kind of data more \\nefficiently.  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='856d73a8-49ad-4280-ad3e-a867ef74db23', embedding=None, metadata={'page_label': '2', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Recurrent Neural Networks (RNN)  \\nA Recurrent Neural Network (RNN)  is a type of neural network that remembers past information  and \\nuses it to inform the current output . This makes RNNs great for sequential data , where the order of \\ninputs matters, like time series, speech, or text data.  \\nIn a standard neural network , we treat each input independently. For example, if you’re processing a \\nsentence, the network would treat each word as an individual input with no memory of the previous word.  \\nIn contrast, an RNN processes a sequence of inputs  (e.g., a sentence word by word) and remembers  \\nthe previous words to help understand the next one. This is crucial when the context or history matters, \\nlike predicting the next word in a sentence.  \\nIn an RNN, each neuron  (or \"cell\") has a loop, which allows it to pass information from one step to the \\nnext. Here’s how it works step -by-step:  \\n \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b1381e7d-573e-4c66-a45c-57eb1352482f', embedding=None, metadata={'page_label': '3', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative Adversarial Networks (GAN)  \\nA Generative Adversarial Network (GAN)  is a type of neural network architecture used to generate new \\ndata that is similar to the data it has been trained on. It’s made up of two networks that compete against \\neach other : a generator  and a discriminator . \\nThe goal of a GAN is to generate realistic -looking data  (like images) that are indistinguishable from real \\ndata.  \\n \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d3a6829-b5e3-4ef1-aa37-148d91f57640', embedding=None, metadata={'page_label': '4', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='207f9d9f-befa-4216-ab6a-a83a09868c2d', embedding=None, metadata={'page_label': '5', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Large Language Models (LLM)  \\nA Large Language Model (LLM)  is a type of neural network designed to understand and generate human \\nlanguage. These models are trained on massive amounts of text data and can perform tasks like \\nanswering questions, generating text, summarizing content, translating languages, and much more.  \\nLLMs are based on the Transformer architecture , which allows them to process and generate text \\nefficiently, even with long sequences of words or sentences.  \\nIn traditional neural networks, each input is processed in a fixed, independent manner, without much \\nconsideration of the relationships between inputs. In contrast, LLMs are built to understand language , \\nmeaning they consider the relationships between words in a sequence and the broader context.  \\nAt the core of an LLM is a Transformer model . The key innovation that makes Transformers powerful is \\ntheir use of attention mechanisms  that allow the model to focus on important parts of the input when \\nmaking predictions. Here’s how it works:  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3439d6d-b7cf-46e1-bd5f-ee448e744d2c', embedding=None, metadata={'page_label': '6', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d54e41f-2583-4758-8ef2-7c135295239f', embedding=None, metadata={'page_label': '7', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generative AI (GenAI)  \\nGenerative AI  refers to a class of artificial intelligence models that are capable of generating new \\ncontent—such as text, images, music, code, or even videos. Instead of just recognizing patterns or \\nmaking predictions like traditional AI, generative AI can create entirely new data that resembles the data \\nit was trained on.  \\nGenerative AI models work by learning the patterns  and structures  in the training data and using that \\nunderstanding to create new examples. The key behind generative AI is that it tries to mimic the data \\ndistribution  of the real -world data it has seen.  \\nGenerative AI models come in different forms, with the most common types being GANs, VAEs \\n(Variational Autoencoders) , and Transformers  (used in LLMs like GPT for text generation).  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='773bddd5-cb78-4ba3-868e-c0d7de552efd', embedding=None, metadata={'page_label': '8', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80ce9465-28bb-4979-8807-a1552f4f48e4', embedding=None, metadata={'page_label': '9', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a166f27c-f557-4695-ba5e-4c3fe2bd0f36', embedding=None, metadata={'page_label': '10', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transformer Models  \\nTransformer models  are a type of neural network architecture designed to handle sequential data , like \\nlanguage, efficiently. They are widely used for tasks like text generation , translation , summarization , \\nand more. The key feature of Transformer models is their use of self-attention mechanisms , which allow \\nthe model to focus on important parts of the input sequence when processing data.  \\nTransformers are the foundation of many state -of-the-art models, including GPT, BERT, and T5. \\n \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86f34831-f4f3-4bf4-9cfb-d33cc003064a', embedding=None, metadata={'page_label': '11', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f67a1fc3-dfec-4423-8abb-7c5e9ad47d99', embedding=None, metadata={'page_label': '12', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Diffusion Models  \\nDiffusion models  are a type of generative model designed to create new data, such as images, by \\ngradually denoising  a random input. They work by learning how to reverse the process of adding noise to \\ndata, such as images, and can generate highly detailed and realistic outputs. Diffusion models have \\nbecome popular in tasks like image generation  and are seen as a powerful alternative to GANs for \\ncertain types of data generation.  \\n \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a946804d-c96c-4e55-925b-1f0d9da20f55', embedding=None, metadata={'page_label': '13', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0b5426c-4c32-492e-9f45-3ef564519d61', embedding=None, metadata={'page_label': '14', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Multimodal Modals  \\nMultimodal models  are AI models designed to handle and integrate multiple types of data —such as text, \\nimages, audio, or video —at the same time. Unlike traditional models that work with just one type of data \\n(like text in language models or images in CNNs), multimodal models  combine different data modalities \\nto generate more context-aware and comprehensive outputs . \\nFor example, a multimodal model might be able to generate text descriptions of an image , answer \\nquestions based on an image , or even produce captions for videos  by understanding both visual and \\ntextual information.  \\nMultimodal models learn how to combine information from different data types  (modalities) by using \\nspecialized architectures, such as Transformers , to understand relationships between different kinds of \\ninputs. This allows them to generate outputs that reflect the combined meaning or insights from multiple \\ntypes of data.  \\nFor instance, a multimodal model might use text and image inputs together to generate a rich description \\nof the image that captures both visual and linguistic context.  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='96576722-cc62-4bef-9d9c-c8331fcd9340', embedding=None, metadata={'page_label': '15', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbbf1141-d69d-4842-8181-04db1045ff7a', embedding=None, metadata={'page_label': '16', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Transfer Learning and Pre -trained Models  \\nTransfer learning  is a technique in machine learning where a model trained on one task is reused or \\nfine-tuned for another related task. Instead of starting from scratch, the model builds on knowledge it \\nhas already learned  from a previous task, making the process faster and more efficient, especially when \\nthere’s limited data for the new task.  \\nThis approach is widely used in modern AI, particularly in large neural networks and pre-trained models , \\nwhere a model is pre -trained on a large dataset (like ImageNet or a large text corpus) and then fine -tuned \\non a smaller, task -specific dataset.  \\nTransfer learning works by transferring knowledge  from a model that has already been trained on one \\ntask to a new, but related, task. This allows the model to leverage pre-learned features  (such as patterns \\nin images or language) for the new task.  \\nFor example, a model trained on millions of images can learn general features like edges, shapes, and \\ntextures. When fine -tuned for a new task (like classifying medical images), these pre -learned features \\nhelp the model quickly adapt without needing as muc h data.  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8348f83-c4a0-4e9c-89a1-3849ccdd922d', embedding=None, metadata={'page_label': '17', 'file_name': 'Neural Networks and AI Advances.pdf', 'file_path': 'd:\\\\pythonProjects\\\\RAG_finetuning\\\\data\\\\Neural Networks and AI Advances.pdf', 'file_type': 'application/pdf', 'file_size': 773786, 'creation_date': '2024-11-06', 'last_modified_date': '2024-10-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=SimpleDirectoryReader('data').load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
