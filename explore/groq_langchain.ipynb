{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import streamlit\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import dotenv \n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# login hugging face and setup llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\26amr\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "llm=ChatGroq(\n",
    "    groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name='llama-3.1-70b-versatile'\n",
    ")\n",
    "login(token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the data into chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "# import faiss\n",
    "# embeddings=OllamaEmbeddings()\n",
    "# loader=WebBaseLoader('https://en.wikipedia.org/wiki/Elon_Musk')\n",
    "# docs=loader.load()\n",
    "# text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "# documents=text_splitter.split_documents(docs)\n",
    "# print(documents)\n",
    "\n",
    "documents=SimpleDirectoryReader('../data/').load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "d=1536\n",
    "faiss_index=faiss.IndexFlatL2(d)\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import Settings\n",
    "embed_model=HuggingFaceEmbedding(\n",
    "    model_name='BAAI/bge-small-en-v1.5'\n",
    ")\n",
    "# document_text=[doc.page_content for doc in documents]\n",
    "# embeddings=[embed_model.embed_query(text) for text in document_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.chunk_size=1024\n",
    "Settings.embed_model=embed_model\n",
    "Settings.llm=llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=VectorStoreIndex.from_documents(documents=documents,embed_model=Settings.embed_model)\n",
    "# save the data \n",
    "index.storage_context.persist('../data/')\n",
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Attorney General is mentioned in the provided text as the authority who provides opinions and guidance on various matters. There are two Attorney General's opinions mentioned. \\n\\nOne opinion is from C.A. Culberson, who is of the opinion that the land department of a railroad company is not considered one of the general offices within the meaning of the statutes. This opinion is based on the history and policy of the legislation and the specific facts of the case.\\n\\nThe other opinion is from Frank Andrews, Office Assistant Attorney General, who is of the opinion that a judgment against an original land purchaser is void if the purchaser had properly transferred the land and the transfer was on file in the General Land office prior to the institution of the suit. Andrews also provides an opinion on the constitutionality of a law related to the Penitentiary Board's authority to purchase lands for the purpose of establishing State farms and employing convict labor.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query('summarize on attorney general ')\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding the chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "from_documents",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      5\u001b[0m embed_model\u001b[38;5;241m=\u001b[39mHuggingFaceEmbedding(\n\u001b[0;32m      6\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/bge-small-en-v1.5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m vector_store\u001b[38;5;241m=\u001b[39m\u001b[43mFaissVectorStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m(documents,embed_model)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# from llama_index import FAISS\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# from llaam.embeddings.huggingface import HuggingFaceEmbedding\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"FAISS index created successfully with the HuggingFace embedding model.\")\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\genai\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py:262\u001b[0m, in \u001b[0;36mModelMetaclass.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mand\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m private_attributes:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m private_attributes[item]\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n",
      "\u001b[1;31mAttributeError\u001b[0m: from_documents"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import Settings\n",
    "embed_model=HuggingFaceEmbedding(\n",
    "    model_name='BAAI/bge-small-en-v1.5'\n",
    ")\n",
    "# vector_store=FaissVectorStore.from_documents(documents,embed_model)\n",
    "\n",
    "# from llama_index import FAISS\n",
    "# from llaam.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# # Initialize the embedding model\n",
    "# embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# # Ensure your documents are loaded in a list of text (e.g., `documents` is a list of strings)\n",
    "# # If your documents are more complex objects, extract their text content first\n",
    "# document_texts = [doc.page_content for doc in documents]  # Adjust if needed\n",
    "\n",
    "# # Generate embeddings for each document text\n",
    "# embeddings = [embed_model.embed_query(text) for text in document_texts]  # Using embed_query here\n",
    "\n",
    "# # Create the FAISS index from the embeddings\n",
    "# vectors = FAISS.from_embeddings(embeddings, metadatas=[doc.metadata for doc in documents])\n",
    "\n",
    "# print(\"FAISS index created successfully with the HuggingFace embedding model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    You are an experienced immigration lawyer providing legal advice to an immigrant client. Analyze the context and documents carefully, considering all possible steps, risks, and requirements for the client's immigration status. \n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "<Documents>\n",
    "{documents}\n",
    "</Documents>\n",
    "\n",
    "Considerations:\n",
    "1. Identify and clarify the primary legal issues affecting the client's case.\n",
    "2. Provide a step-by-step overview of the necessary actions the client should take, considering both short-term and long-term implications.\n",
    "3. Explain the potential legal outcomes and any significant risks or benefits.\n",
    "4. Highlight any documents, forms, or evidence the client will need to prepare or retain.\n",
    "5. Offer any additional advice on maintaining compliance with immigration laws or procedures.\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Please provide a clear, structured response that includes legal options, procedural steps, and any supporting advice for the client.\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m document_chain\u001b[38;5;241m=\u001b[39m\u001b[43mcre\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cre' is not defined"
     ]
    }
   ],
   "source": [
    "document_chain=cre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
